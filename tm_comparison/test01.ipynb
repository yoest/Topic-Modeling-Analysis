{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from ris_evaluation.evaluator import Evaluator\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "\n",
    "from gensim import corpora\n",
    "import gensim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "\n",
    "    def __init__(self, dataset_name: str) -> None:\n",
    "        self.dataset_name = dataset_name\n",
    "\n",
    "        self.documents_df = pd.read_csv(f'../../datasets/data/{dataset_name}/documents.csv')\n",
    "        self.documents_df = self.documents_df\n",
    "\n",
    "        self.documents = self.documents_df['document'].tolist()\n",
    "        self.labels = self.documents_df['class_name'].tolist()\n",
    "\n",
    "        self.labels_df = pd.read_csv(f'../../datasets/data/{dataset_name}/labels.csv')\n",
    "        self.defined_keywords = [keywords.split(' ') for keywords in self.labels_df['class_keywords'].tolist()]\n",
    "\n",
    "        self.num_topics = len(set(self.labels))\n",
    "        self.random_n_iter = 5\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\" Train the model \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def get_output(self):\n",
    "        \"\"\" Get the output of the model on the OCTIS format \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def get_results_df(self):\n",
    "        \"\"\" Get the results of the model on a DataFrame \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def get_words_for_topics(self, topics):\n",
    "        \"\"\" Get the words for each topic from the documents\n",
    "\n",
    "        Args:\n",
    "            topics (list): The topics for each document\n",
    "\n",
    "        Returns:\n",
    "            dict: The words for each topic\n",
    "        \"\"\"\n",
    "        words_by_topics = {}\n",
    "        for idx, topic in enumerate(topics):\n",
    "            words = self.documents_df.iloc[idx]['document'].split()\n",
    "\n",
    "            if topic not in words_by_topics:\n",
    "                words_by_topics[topic] = {}\n",
    "\n",
    "            for word in words:\n",
    "                if word not in words_by_topics[topic]:\n",
    "                    words_by_topics[topic][word] = 0\n",
    "\n",
    "                words_by_topics[topic][word] += 1\n",
    "\n",
    "        return words_by_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDAModel(Model):\n",
    "\n",
    "    def __init__(self, dataset_name: str) -> None:\n",
    "        super().__init__(dataset_name)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\" See the documentation of the parent class \"\"\"\n",
    "        self.vectorizer = CountVectorizer()\n",
    "        X = self.vectorizer.fit_transform(self.documents)\n",
    "\n",
    "        self.lda = LatentDirichletAllocation(n_components=self.num_topics)\n",
    "        self.lda.fit(X)\n",
    "\n",
    "    def get_results_df(self):\n",
    "        \"\"\" See the documentation of the parent class \"\"\"\n",
    "        results_df = pd.DataFrame()\n",
    "        results_df['document'] = self.documents\n",
    "        results_df['y_true'] = self.labels\n",
    "\n",
    "        X = self.vectorizer.transform(self.documents)\n",
    "        results_df['y_pred'] = self.lda.transform(X).argmax(axis=1)\n",
    "        results_df['y_pred_highest_proba'] = self.lda.transform(X).max(axis=1)\n",
    "        return results_df\n",
    "\n",
    "    def get_output(self):\n",
    "        \"\"\" See the documentation of the parent class \"\"\"\n",
    "        topics = []\n",
    "        for topic in self.lda.components_:\n",
    "            topic_words = []\n",
    "            for i in topic.argsort()[-10:]:\n",
    "                topic_words.append(self.vectorizer.get_feature_names_out()[i])\n",
    "            topics.append(topic_words)\n",
    "\n",
    "        return {\n",
    "            \"topics\": topics,\n",
    "            \"topic-document-matrix\": None,\n",
    "            \"topic-word-matrix\": None,\n",
    "            \"test-topic-document-matrix\": None\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTopicModel(Model):\n",
    "\n",
    "    def __init__(self, dataset_name: str) -> None:\n",
    "        super().__init__(dataset_name)\n",
    "        self.num_topics = len(set(self.labels)) + 1  # +1 for outliers\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\" See the documentation of the parent class \"\"\"\n",
    "        self.bert_model = BERTopic(language=\"english\", calculate_probabilities=True, nr_topics=self.num_topics)\n",
    "        self.topics, self.probs = self.bert_model.fit_transform(self.documents)\n",
    "\n",
    "    def get_results_df(self):\n",
    "        \"\"\" See the documentation of the parent class \"\"\"\n",
    "        results_df = pd.DataFrame()\n",
    "        results_df['document'] = self.documents\n",
    "        results_df['y_true'] = self.labels\n",
    "\n",
    "        results_df['y_pred'] = self.topics\n",
    "        results_df['y_pred_highest_proba'] = np.max(self.probs, axis=1)\n",
    "\n",
    "        relevant_results_df = results_df[results_df['y_pred'] != -1]\n",
    "        return relevant_results_df\n",
    "    \n",
    "    def get_output(self):\n",
    "        \"\"\" See the documentation of the parent class \"\"\"\n",
    "        return {\n",
    "            \"topics\": [item for item in self.bert_model.get_topic_info()[\"Representation\"]],\n",
    "            \"topic-document-matrix\": self.probs.transpose(),\n",
    "            \"topic-word-matrix\": self.bert_model.c_tf_idf_,\n",
    "            \"test-topic-document-matrix\": self.probs.transpose()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuidedLDA(Model):\n",
    "\n",
    "    def __init__(self, dataset_name: str) -> None:\n",
    "        super().__init__(dataset_name)\n",
    "\n",
    "    def train(self) -> None:\n",
    "        \"\"\" See the documentation of the parent class \"\"\"\n",
    "        self.texts = [document.split(' ') for document in self.documents]\n",
    "        \n",
    "        self.dictionary = corpora.Dictionary(self.texts)\n",
    "        self.corpus = [self.dictionary.doc2bow(text) for text in self.texts]\n",
    "\n",
    "        priors = {}\n",
    "        for idx, keywords in enumerate(self.defined_keywords):\n",
    "            for keyword in keywords:\n",
    "                priors[keyword] = idx\n",
    "\n",
    "        eta = np.full(shape=(self.num_topics, len(self.dictionary)), fill_value=1) # create a (ntopics, nterms) matrix and fill with 1\n",
    "        for word, topic in priors.items(): # for each word in the list of priors\n",
    "            keyindex = [index for index,term in self.dictionary.items() if term == word] # look up the word in the dictionary\n",
    "            if (len(keyindex) > 0): # if it's in the dictionary\n",
    "                eta[topic,keyindex[0]] = 1e7  # put a large number in there\n",
    "        eta = np.divide(eta, eta.sum(axis=0)) # normalize so that the probabilities sum to 1 over all topics\n",
    "\n",
    "        with (np.errstate(divide='ignore')):  # ignore divide-by-zero warnings\n",
    "            self.model = gensim.models.ldamodel.LdaModel(\n",
    "                corpus=self.corpus, id2word=self.dictionary, num_topics=self.num_topics,\n",
    "                random_state=42, chunksize=100, eta=eta,\n",
    "                eval_every=-1, update_every=1,\n",
    "                passes=150, alpha='auto', per_word_topics=True)\n",
    "\n",
    "    def get_results_df(self):\n",
    "        \"\"\" See the documentation of the parent class \"\"\"\n",
    "        self.results_df = pd.DataFrame()\n",
    "        self.results_df['document'] = self.documents\n",
    "        self.results_df['y_true'] = self.documents_df['class_name'].tolist()\n",
    "\n",
    "        scores = [[value[1] for value in score_values[0]] for score_values in self.model[self.corpus]]\n",
    "        self.results_df['y_pred'] = [np.argmax(score) for score in scores]\n",
    "        self.results_df['y_pred_highest_proba'] = [np.max(score) for score in scores]\n",
    "        return self.results_df\n",
    "    \n",
    "    def get_output(self):\n",
    "        \"\"\" See the documentation of the parent class \"\"\"\n",
    "        return {\n",
    "            \"topics\": [self.model.show_topic(topicid, topn=10) for topicid in range(self.num_topics)],\n",
    "            \"topic-document-matrix\": None,\n",
    "            \"topic-word-matrix\": None,\n",
    "            \"test-topic-document-matrix\": None\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuidedBERTopicModel(BERTopicModel):\n",
    "\n",
    "    def __init__(self, dataset_name: str) -> None:\n",
    "        super().__init__(dataset_name)\n",
    "\n",
    "    def train(self) -> None:\n",
    "        \"\"\" See the documentation of the parent class \"\"\"\n",
    "        umap = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', low_memory=False, random_state=0)\n",
    "        self.model = BERTopic(language=\"english\", calculate_probabilities=True, nr_topics=self.num_topics, umap_model=umap, seed_topic_list=self.defined_keywords)\n",
    "        self.model.fit(self.documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_df(dataset_names, models, n_iterations=10):\n",
    "    metrics_df = pd.DataFrame()\n",
    "    idx = 0\n",
    "\n",
    "    for dataset in dataset_names:\n",
    "        # -- Get dataset\n",
    "        documents_df = pd.read_csv(f'../datasets/data/{dataset}/documents.csv')\n",
    "\n",
    "        for model in models:\n",
    "            avg_results = {}\n",
    "\n",
    "            for i in range(n_iterations):\n",
    "                print(f'Iteration {i+1}/{n_iterations} for {dataset} and {model.__name__}', end='\\r')\n",
    "\n",
    "                # -- Train model\n",
    "                trained_model = model(documents_df)\n",
    "                trained_model.train()\n",
    "\n",
    "                model_output = trained_model.get_output()\n",
    "\n",
    "                # -- Evaluate model\n",
    "                evaluator = Evaluator(model_output)\n",
    "                results_df = trained_model.get_results_df()\n",
    "\n",
    "                words_by_extracted_topics = trained_model.get_words_for_topics(results_df['y_pred'].tolist())\n",
    "                words_by_class = trained_model.get_words_for_topics(results_df['y_true'].tolist())\n",
    "\n",
    "                coherence = evaluator.compute_coherence()\n",
    "                diversity = evaluator.compute_diversity()\n",
    "                supervised_correlation = evaluator.compute_supervised_correlation(words_by_extracted_topics, words_by_class)\n",
    "\n",
    "                # -- Average results\n",
    "                for coherence_type, coherence_value in coherence.items():\n",
    "                    if coherence_type not in avg_results:\n",
    "                        avg_results[f'coherence_{coherence_type}'] = []\n",
    "                    avg_results[f'coherence_{coherence_type}'].append(coherence_value)\n",
    "                \n",
    "                avg_results['diversity'] = avg_results.get('diversity', []) + [diversity]\n",
    "                avg_results['supervised_correlation'] = avg_results.get('supervised_correlation', []) + [supervised_correlation]\n",
    "\n",
    "            for key, value in avg_results.items():\n",
    "                avg_results[key] = np.mean(value)\n",
    "\n",
    "            metrics_results = {}\n",
    "            metrics_results['dataset'] = dataset\n",
    "            metrics_results['model'] = trained_model.__class__.__name__\n",
    "            for key, value in avg_results.items():\n",
    "                metrics_results[key] = [value]\n",
    "\n",
    "            metrics_df = pd.concat([metrics_df, pd.DataFrame(metrics_results, index=[idx])])\n",
    "            idx += 1\n",
    "\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [LDAModel, BERTopicModel]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BBC News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Iteration 3/5 for M10 and BERTopicModel\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yorest/miniconda3/envs/nlp/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/yorest/miniconda3/envs/nlp/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5/5 for M10 and BERTopicModel\r"
     ]
    }
   ],
   "source": [
    "bbc_news_metrics_df = compute_metrics_df(dataset_names=['BBC_News'], models=models, n_iterations=5)\n",
    "print(bbc_news_metrics_df.to_latex(index=False))\n",
    "bbc_news_metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20NewsGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_metrics_df = compute_metrics_df(dataset_names=['20NewsGroup'], models=models, n_iterations=5)\n",
    "print(newsgroups_metrics_df.to_latex(index=False))\n",
    "newsgroups_metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dblp_metrics_df = compute_metrics_df(dataset_names=['DBLP'], models=models, n_iterations=5)\n",
    "print(dblp_metrics_df.to_latex(index=False))\n",
    "dblp_metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m10_metrics_df = compute_metrics_df(dataset_names=['M10'], models=models, n_iterations=5)\n",
    "print(m10_metrics_df.to_latex(index=False))\n",
    "m10_metrics_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
