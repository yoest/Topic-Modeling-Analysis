{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from lbl2vec import Lbl2Vec, Lbl2TransformerVec\n",
    "\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import strip_tags\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spacy_similar_words_for_dataset(dataset_name, save_cache=False):\n",
    "    documents_df = pd.read_csv(f'../datasets/data/{dataset_name}/documents.csv')\n",
    "    labels_df = pd.read_csv(f'../datasets/data/{dataset_name}/labels.csv')\n",
    "\n",
    "    class_keywords = labels_df['class_keywords'].values.tolist()\n",
    "\n",
    "    spacy_similarity_model = spacy.load('en_core_web_lg')\n",
    "    resulting_keywords = {}\n",
    "\n",
    "    all_documents_words = []\n",
    "    for doc in documents_df[documents_df['dataset_type'] == 'train']['document']:\n",
    "        all_documents_words.extend(doc.split(' '))\n",
    "    all_documents_words = list(set(all_documents_words))\n",
    "\n",
    "    for class_keyword in class_keywords:\n",
    "        print(f'[INFO] Processing class keywords: {class_keyword}...' + ' ' * 20, end='\\r')\n",
    "\n",
    "        similarity_levels = []\n",
    "        \n",
    "        for word in all_documents_words:\n",
    "            spacy_word_1 = spacy_similarity_model(word)\n",
    "            spacy_word_2 = spacy_similarity_model(class_keyword)\n",
    "\n",
    "            if(not (spacy_word_1 and spacy_word_1.vector_norm and spacy_word_2 and spacy_word_2.vector_norm)):\n",
    "                continue\n",
    "            similarity_level = spacy_word_1.similarity(spacy_word_2)\n",
    "\n",
    "            similarity_levels.append((word, similarity_level))\n",
    "\n",
    "        similarity_levels = sorted(similarity_levels, key=lambda x: x[1], reverse=True)\n",
    "        resulting_keywords[class_keyword] = [word for word, _ in similarity_levels]\n",
    "        \n",
    "    if save_cache:\n",
    "        cache_df = pd.DataFrame()\n",
    "        cache_df['class_raw_keywords'] = labels_df['class_keywords']\n",
    "        cache_df['class_result_keywords'] = resulting_keywords.values()\n",
    "        cache_df.to_csv(f'cache/{dataset_name}_spacy_keywords.csv', index=False)\n",
    "\n",
    "    return resulting_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processing class keywords: industrial engineering...                     \r"
     ]
    }
   ],
   "source": [
    "for dataset_name in ['20NewsGroup', 'DBLP', 'M10']:\n",
    "    get_spacy_similar_words_for_dataset(dataset_name, save_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_lbl2vec_on_dataset(dataset_name: str, n_iterations: int = 10, n_words: int = 1):\n",
    "    def tokenize(doc):\n",
    "        return simple_preprocess(strip_tags(doc), deacc=True, min_len=2, max_len=15)\n",
    "    \n",
    "    # -- Documents\n",
    "    documents_df = pd.read_csv(f'../datasets/data/{dataset_name}/documents.csv')\n",
    "    documents_df['tagged_docs'] = documents_df.apply(\n",
    "        lambda row: TaggedDocument(tokenize(row['document']), [str(row.name)]), axis=1\n",
    "    )\n",
    "    documents_df['doc_key'] = documents_df.index.astype(str)\n",
    "\n",
    "    # -- Labels\n",
    "    labels_df = pd.read_csv(f'../datasets/data/{dataset_name}/labels.csv')\n",
    "    labels_df['number_of_keywords'] = labels_df['class_keywords'].apply(lambda keywords: len(keywords))\n",
    "\n",
    "    keywords_cached_df = pd.read_csv(f'cache/{dataset_name}_spacy_keywords.csv')\n",
    "    keywords_cached_df['class_result_keywords'] = keywords_cached_df['class_result_keywords'].apply(lambda x: x[1:-1].replace(\"'\", '').split(', '))\n",
    "    similar_keywords = dict(zip(keywords_cached_df['class_raw_keywords'], keywords_cached_df['class_result_keywords']))\n",
    "    labels_df['class_keywords'] = labels_df.apply(\n",
    "        lambda row: similar_keywords[row['class_keywords']][:n_words], axis=1\n",
    "    )\n",
    "\n",
    "    # -- Model\n",
    "    lbl2vec_model = Lbl2Vec(\n",
    "        keywords_list=[item for item in labels_df['class_keywords']], \n",
    "        tagged_documents=list(documents_df['tagged_docs'][documents_df['dataset_type'] == 'train']), \n",
    "        label_names=list(labels_df['class_name']),\n",
    "        min_count=2,\n",
    "        verbose=False\n",
    "    )\n",
    "    lbl2vec_model.fit()\n",
    "\n",
    "    # -- Evaluation\n",
    "    avg_f1_score_train = 0\n",
    "    avg_f1_score_test = 0\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        print(f'[INFO] Iteration {i+1}/{n_iterations}...' + ' ' * 20, end='\\r')\n",
    "\n",
    "        train_docs_lbl_similarities = lbl2vec_model.predict_model_docs()\n",
    "        test_docs_lbl_similarities = lbl2vec_model.predict_new_docs(tagged_docs=documents_df['tagged_docs'][documents_df['dataset_type'] == 'test'])\n",
    "\n",
    "        evaluation_train = train_docs_lbl_similarities.merge(documents_df[documents_df['dataset_type']=='train'], left_on='doc_key', right_on='doc_key')\n",
    "        evaluation_test = test_docs_lbl_similarities.merge(documents_df[documents_df['dataset_type']=='test'], left_on='doc_key', right_on='doc_key')\n",
    "\n",
    "        y_true_train = evaluation_train['class_name']\n",
    "        y_pred_train = evaluation_train['most_similar_label']\n",
    "\n",
    "        y_true_test = evaluation_test['class_name']\n",
    "        y_pred_test = evaluation_test['most_similar_label']\n",
    "\n",
    "        avg_f1_score_train += f1_score(y_true_train, y_pred_train, average='micro')\n",
    "        avg_f1_score_test += f1_score(y_true_test, y_pred_test, average='micro')\n",
    "\n",
    "    avg_f1_score_train /= n_iterations\n",
    "    avg_f1_score_test /= n_iterations\n",
    "\n",
    "    return avg_f1_score_train, avg_f1_score_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
