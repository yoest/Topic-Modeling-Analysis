{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from lbl2vec import Lbl2Vec, Lbl2TransformerVec\n",
    "\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import strip_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_lbl2vec_on_dataset(dataset_name: str, n_iterations: int = 1, n_words: int = 1):\n",
    "    def tokenize(doc):\n",
    "        return simple_preprocess(strip_tags(doc), deacc=True, min_len=2, max_len=15)\n",
    "    \n",
    "    # -- Documents\n",
    "    documents_df = pd.read_csv(f'../datasets/data/{dataset_name}/documents.csv')\n",
    "    documents_df['tagged_docs'] = documents_df.apply(\n",
    "        lambda row: TaggedDocument(tokenize(row['document']), [str(row.name)]), axis=1\n",
    "    )\n",
    "    documents_df['doc_key'] = documents_df.index.astype(str)\n",
    "\n",
    "    # -- Labels\n",
    "    labels_df = pd.read_csv(f'../datasets/data/{dataset_name}/labels.csv')\n",
    "    labels_df['number_of_keywords'] = labels_df['class_keywords'].apply(lambda keywords: len(keywords))\n",
    "\n",
    "    keywords_cached_df = pd.read_csv(f'cache/{dataset_name}_spacy_keywords.csv')\n",
    "    keywords_cached_df['class_result_keywords'] = keywords_cached_df['class_result_keywords'].apply(lambda x: x[1:-1].replace(\"'\", '').split(', '))\n",
    "    similar_keywords = dict(zip(keywords_cached_df['class_raw_keywords'], keywords_cached_df['class_result_keywords']))\n",
    "    labels_df['class_result_keywords'] = labels_df.apply(\n",
    "        lambda row: similar_keywords[row['class_keywords']][:n_words], axis=1\n",
    "    )\n",
    "    documents_df['class_keywords'] = documents_df['class_name'].apply(lambda class_name: labels_df[labels_df['class_name'] == class_name]['class_keywords'].values[0])\n",
    "\n",
    "    # -- Evaluation\n",
    "    avg_f1_score_train = 0\n",
    "    avg_f1_score_test = 0\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        print(f'[INFO] {dataset_name} | Iteration {i+1}/{n_iterations}...' + ' ' * 20, end='\\r')\n",
    "\n",
    "        # -- Model\n",
    "        lbl2vec_model = Lbl2Vec(\n",
    "            keywords_list=[item for item in labels_df['class_result_keywords']], \n",
    "            tagged_documents=list(documents_df['tagged_docs'][documents_df['dataset_type'] == 'train']), \n",
    "            label_names=[item for item in labels_df['class_keywords']], \n",
    "            min_count=2,\n",
    "            verbose=False\n",
    "        )\n",
    "        lbl2vec_model.fit()\n",
    "\n",
    "        train_docs_lbl_similarities = lbl2vec_model.predict_model_docs()\n",
    "        test_docs_lbl_similarities = lbl2vec_model.predict_new_docs(tagged_docs=documents_df['tagged_docs'][documents_df['dataset_type'] == 'test'])\n",
    "\n",
    "        evaluation_train = train_docs_lbl_similarities.merge(documents_df[documents_df['dataset_type']=='train'], left_on='doc_key', right_on='doc_key')\n",
    "        evaluation_test = test_docs_lbl_similarities.merge(documents_df[documents_df['dataset_type']=='test'], left_on='doc_key', right_on='doc_key')\n",
    "\n",
    "        y_true_train = evaluation_train['class_keywords']\n",
    "        y_pred_train = evaluation_train['most_similar_label']\n",
    "\n",
    "        y_true_test = evaluation_test['class_keywords']\n",
    "        y_pred_test = evaluation_test['most_similar_label']\n",
    "\n",
    "        current_f1_score_train = f1_score(y_true_train, y_pred_train, average='micro')\n",
    "        current_f1_score_test = f1_score(y_true_test, y_pred_test, average='micro')\n",
    "\n",
    "        avg_f1_score_train += current_f1_score_train\n",
    "        avg_f1_score_test += current_f1_score_test\n",
    "\n",
    "        print(f'[INFO] Iteration {i+1}/{n_iterations} - Train F1 Score: {current_f1_score_train:.4f} - Test F1 Score: {current_f1_score_test:.4f}' + ' ' * 10, end='\\r')\n",
    "\n",
    "    avg_f1_score_train /= n_iterations\n",
    "    avg_f1_score_test /= n_iterations\n",
    "\n",
    "    return avg_f1_score_train, avg_f1_score_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Iteration 10/10 - Train F1 Score: 0.8685 - Test F1 Score: 0.8494          \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7860674157303371, 0.7714606741573033)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_lbl2vec_on_dataset('BBC_News', n_iterations=10, n_words=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
